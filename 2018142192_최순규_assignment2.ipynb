{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1632618716467,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "NZZzm8PY2gNR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rjuQY9f2mdS"
   },
   "source": [
    "## Assignment 2-1\n",
    "ReLu activation function을 구현해보세요\n",
    "\n",
    "- Hint : np.maximum 함수 사용하면 편리합니다\n",
    "- 다른 방법 사용하셔도 무방합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1632618800951,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "puH0YVGI2uLz"
   },
   "outputs": [],
   "source": [
    "#def relu(x):\n",
    "#\n",
    "#  return\n",
    "\n",
    "def relu(x):\n",
    "  return np.maximum(0,x) #0과 x값 중 큰 값을 반환해 음수 부분에선 모두 0이 나오고 양수 부분에선 x를 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wz8Hi0Rc2-yJ"
   },
   "source": [
    "##Assignment 2-2\n",
    "ReLu의 derivative function을 구현해보세요\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632618803406,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "fusEy49j3uhs"
   },
   "outputs": [],
   "source": [
    "#def d_relu(x):\n",
    "#\n",
    "#  return\n",
    "def d_relu(x):\n",
    "  x[x<=0]=0\n",
    "  x[x>0]=1\n",
    "  return x #x<0에서 relu function은 y=0상수함수와 같은 개형으로 기울기가 0이고, x>0에서는 기울기가 1이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twf-R8s-34zT"
   },
   "source": [
    "## Assignment 2-3\n",
    "Lecture 2의 2. Backpropagation with numpy 코드 참고해서\n",
    "Three layer MLP를 구한후, 학습을 돌려 보세요\n",
    "\n",
    "hyperparameter는 다음과 같이 설정\n",
    "\n",
    "- <#> of train data, <#> of test data : 60000, 10000\n",
    "- epochs : 100\n",
    "- hiddensize : 128, 64 (two layer)\n",
    "- learning_rate : 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 35363,
     "status": "ok",
     "timestamp": 1632618841007,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "uwtC3cI_Uyhb"
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 329,
     "status": "ok",
     "timestamp": 1632618842901,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "AzsbCMLAU0ZC"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "\n",
    "num_train = 60000\n",
    "num_class = 10\n",
    "\n",
    "x_train = np.float32(mnist.data[:num_train]).T\n",
    "y_train_index = np.int32(mnist.target[:num_train]).T\n",
    "x_test = np.float32(mnist.data[num_train:]).T\n",
    "y_test_index = np.int32(mnist.target[num_train:]).T\n",
    "\n",
    "# Normalization\n",
    "\n",
    "x_train /= 255 #8비트 범위이기 때문에 모든 값을 0~1사이로 바꿀 수 있다.\n",
    "x_test /= 255\n",
    "x_size = x_train.shape[0]\n",
    "\n",
    "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
    "for idx in range(y_train_index.shape[0]):\n",
    "  y_train[y_train_index[idx], idx] = 1\n",
    "\n",
    "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
    "for idx in range(y_test_index.shape[0]):\n",
    "  y_test[y_test_index[idx], idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632618844236,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "cQ0P7XcuVLjY"
   },
   "outputs": [],
   "source": [
    "def relu(x): #성능 향상을 위해 활성화함수로는 sigmoid가 아니라 2.1, 2.2에서 만든 relu를 사용하였다.\n",
    "  return np.maximum(0,x)\n",
    "\n",
    "def d_relu(x):  \n",
    "  x[x<=0]=0\n",
    "  x[x>0]=1\n",
    "  return x\n",
    "\n",
    "def softmax(x):\n",
    "  exp = np.exp(x)\n",
    "  return exp/np.sum(exp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632618845445,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "c5x8BG5XVL11"
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "  # loss calculation\n",
    "\n",
    "  num_sample = y_true.shape[1]\n",
    "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
    "  \n",
    "  return Li/num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632618846672,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "bxJO249A3jhk"
   },
   "outputs": [],
   "source": [
    "# Assignment 2-3 구현은 여기서 ()\n",
    "\n",
    "hidden_size_1 = 128\n",
    "hidden_size_2 = 64\n",
    "\n",
    "params={\"W1\": np.random.randn(hidden_size_1, x_size)*np.sqrt(1/x_size),\n",
    "        \"b1\": np.zeros((hidden_size_1, 1))*np.sqrt(1/x_size),\n",
    "        \"W2\": np.random.randn(hidden_size_2, hidden_size_1)*np.sqrt(1/hidden_size_1),\n",
    "        \"b2\": np.zeros((hidden_size_2, 1))*np.sqrt(1/hidden_size_1),\n",
    "        \"W3\": np.random.randn(num_class, hidden_size_2)*np.sqrt(1/hidden_size_2),\n",
    "        \"b3\": np.zeros((num_class, 1))*np.sqrt(1/hidden_size_2)\n",
    "        }\n",
    "        #w,b쌍을 3개 만들어 3 layer MLP를 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1632618851980,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "A854c71GVAYr"
   },
   "outputs": [],
   "source": [
    "def foward_pass(x, params):\n",
    "  \n",
    "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "  params[\"A1\"] = relu(params[\"S1\"])\n",
    "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
    "  params[\"A2\"] = relu(params[\"S2\"])\n",
    "  params[\"S3\"] = np.dot(params[\"W3\"], params[\"A2\"]) + params[\"b3\"]\n",
    "  params[\"A3\"] = softmax(params[\"S3\"])\n",
    "  #위의 두 번의 경우 activation function을 적용한 이후 다시 Wx+b를 시행해주고 마지막에 softmax를 적용해준다.\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632618853365,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "3Dc770LSVAnE"
   },
   "outputs": [],
   "source": [
    "def foward_pass_test(x, params): #위의 foward_pass와 동일하다\n",
    "\n",
    "  params_test = {}\n",
    "  \n",
    "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "  params_test[\"A1\"] = relu(params_test[\"S1\"])\n",
    "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
    "  params_test[\"A2\"] = relu(params_test[\"S2\"])\n",
    "  params_test[\"S3\"] = np.dot(params[\"W3\"], params_test[\"A2\"]) + params[\"b3\"]\n",
    "  params_test[\"A3\"] = softmax(params_test[\"S3\"])\n",
    "\n",
    "  return params_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632618854699,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "kPv4yK1vVA1-"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "  y_true_idx = np.argmax(y_true, axis = 0)\n",
    "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
    "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
    "\n",
    "  accuracy = num_correct / y_true.shape[1] * 100\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632618857264,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "Tgpxq6xpVFDu"
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y_true, params):\n",
    "\n",
    "  dS3 = params[\"A3\"] - y_true\n",
    "  # Please check http://machinelearningmechanic.com/deep_learning/2019/09/04/cross-entropy-loss-derivative.html\n",
    "  # dS2 is softmax + CE loss derivative\n",
    "\n",
    "  grads = {}\n",
    "\n",
    "  grads[\"dW3\"] = np.dot(dS3, params[\"A2\"].T)/x.shape[1]\n",
    "  grads[\"db3\"] = np.sum(dS3, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA2 = np.dot(params[\"W3\"].T, dS3)\n",
    "  dS2 = dA2 * d_relu(params[\"S2\"])\n",
    "\n",
    "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
    "  grads[\"db2\"] = np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
    "  dS1 = dA1 * d_relu(params[\"S1\"])\n",
    "\n",
    "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
    "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 131546,
     "status": "ok",
     "timestamp": 1632618990743,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "VCbRBTuBVFhG",
    "outputId": "b0bdc2c4-69cf-4b1a-e262-59b586424ef0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 2.226837, training acuracy = 27.44%, test loss = 2.228348, test acuracy = 26.93%\n",
      "Epoch 2: training loss = 2.126968, training acuracy = 39.44%, test loss = 2.128151, test acuracy = 39.18%\n",
      "Epoch 3: training loss = 2.003261, training acuracy = 48.12%, test loss = 2.00336, test acuracy = 47.69%\n",
      "Epoch 4: training loss = 1.854638, training acuracy = 53.23%, test loss = 1.853358, test acuracy = 52.96%\n",
      "Epoch 5: training loss = 1.685719, training acuracy = 62.4%, test loss = 1.680317, test acuracy = 63.17%\n",
      "Epoch 6: training loss = 1.515831, training acuracy = 65.68%, test loss = 1.509156, test acuracy = 65.79%\n",
      "Epoch 7: training loss = 1.551183, training acuracy = 53.66%, test loss = 1.545038, test acuracy = 54.12%\n",
      "Epoch 8: training loss = 2.575615, training acuracy = 28.9%, test loss = 2.615075, test acuracy = 29.06%\n",
      "Epoch 9: training loss = 2.275219, training acuracy = 42.66%, test loss = 2.230447, test acuracy = 43.68%\n",
      "Epoch 10: training loss = 1.787015, training acuracy = 37.66%, test loss = 1.784167, test acuracy = 37.5%\n",
      "Epoch 11: training loss = 1.541427, training acuracy = 52.05%, test loss = 1.521191, test acuracy = 52.68%\n",
      "Epoch 12: training loss = 1.420471, training acuracy = 58.98%, test loss = 1.409915, test acuracy = 58.4%\n",
      "Epoch 13: training loss = 1.372341, training acuracy = 51.83%, test loss = 1.348378, test acuracy = 51.59%\n",
      "Epoch 14: training loss = 1.598748, training acuracy = 47.06%, test loss = 1.597556, test acuracy = 45.99%\n",
      "Epoch 15: training loss = 1.506691, training acuracy = 46.46%, test loss = 1.497418, test acuracy = 46.58%\n",
      "Epoch 16: training loss = 1.446859, training acuracy = 45.81%, test loss = 1.44558, test acuracy = 44.62%\n",
      "Epoch 17: training loss = 1.082899, training acuracy = 68.83%, test loss = 1.06429, test acuracy = 69.84%\n",
      "Epoch 18: training loss = 0.925298, training acuracy = 70.72%, test loss = 0.903842, test acuracy = 70.63%\n",
      "Epoch 19: training loss = 1.063677, training acuracy = 64.5%, test loss = 1.057446, test acuracy = 64.7%\n",
      "Epoch 20: training loss = 1.030378, training acuracy = 64.62%, test loss = 0.998048, test acuracy = 65.01%\n",
      "Epoch 21: training loss = 0.96765, training acuracy = 69.32%, test loss = 0.952734, test acuracy = 69.94%\n",
      "Epoch 22: training loss = 0.789192, training acuracy = 75.11%, test loss = 0.774159, test acuracy = 75.18%\n",
      "Epoch 23: training loss = 0.736578, training acuracy = 76.32%, test loss = 0.71815, test acuracy = 76.53%\n",
      "Epoch 24: training loss = 0.717564, training acuracy = 75.18%, test loss = 0.698362, test acuracy = 75.82%\n",
      "Epoch 25: training loss = 0.819431, training acuracy = 72.26%, test loss = 0.807809, test acuracy = 72.6%\n",
      "Epoch 26: training loss = 0.663042, training acuracy = 78.43%, test loss = 0.649648, test acuracy = 78.75%\n",
      "Epoch 27: training loss = 0.632069, training acuracy = 78.59%, test loss = 0.613198, test acuracy = 78.83%\n",
      "Epoch 28: training loss = 0.601223, training acuracy = 80.44%, test loss = 0.585338, test acuracy = 80.43%\n",
      "Epoch 29: training loss = 0.679269, training acuracy = 76.96%, test loss = 0.659123, test acuracy = 77.44%\n",
      "Epoch 30: training loss = 0.879337, training acuracy = 72.27%, test loss = 0.874747, test acuracy = 72.06%\n",
      "Epoch 31: training loss = 0.81392, training acuracy = 73.08%, test loss = 0.785551, test acuracy = 74.29%\n",
      "Epoch 32: training loss = 0.744773, training acuracy = 76.48%, test loss = 0.747416, test acuracy = 76.36%\n",
      "Epoch 33: training loss = 0.634022, training acuracy = 77.45%, test loss = 0.616595, test acuracy = 78.08%\n",
      "Epoch 34: training loss = 0.535001, training acuracy = 83.64%, test loss = 0.520463, test acuracy = 83.67%\n",
      "Epoch 35: training loss = 0.488475, training acuracy = 85.02%, test loss = 0.469604, test acuracy = 85.91%\n",
      "Epoch 36: training loss = 0.468479, training acuracy = 85.78%, test loss = 0.451657, test acuracy = 86.39%\n",
      "Epoch 37: training loss = 0.46177, training acuracy = 85.41%, test loss = 0.44504, test acuracy = 85.96%\n",
      "Epoch 38: training loss = 0.471702, training acuracy = 84.85%, test loss = 0.456524, test acuracy = 85.01%\n",
      "Epoch 39: training loss = 0.483745, training acuracy = 83.39%, test loss = 0.470435, test acuracy = 83.87%\n",
      "Epoch 40: training loss = 0.525181, training acuracy = 82.53%, test loss = 0.514355, test acuracy = 82.43%\n",
      "Epoch 41: training loss = 0.509218, training acuracy = 82.14%, test loss = 0.498423, test acuracy = 82.72%\n",
      "Epoch 42: training loss = 0.511784, training acuracy = 82.9%, test loss = 0.499507, test acuracy = 83.02%\n",
      "Epoch 43: training loss = 0.465109, training acuracy = 84.73%, test loss = 0.454216, test acuracy = 85.14%\n",
      "Epoch 44: training loss = 0.447932, training acuracy = 85.57%, test loss = 0.431181, test acuracy = 85.98%\n",
      "Epoch 45: training loss = 0.43445, training acuracy = 86.4%, test loss = 0.425133, test acuracy = 86.59%\n",
      "Epoch 46: training loss = 0.441451, training acuracy = 85.85%, test loss = 0.42324, test acuracy = 86.41%\n",
      "Epoch 47: training loss = 0.434105, training acuracy = 86.42%, test loss = 0.426576, test acuracy = 86.58%\n",
      "Epoch 48: training loss = 0.451142, training acuracy = 85.52%, test loss = 0.433209, test acuracy = 86.05%\n",
      "Epoch 49: training loss = 0.422368, training acuracy = 86.98%, test loss = 0.414631, test acuracy = 87.35%\n",
      "Epoch 50: training loss = 0.412416, training acuracy = 87.06%, test loss = 0.395435, test acuracy = 87.68%\n",
      "Epoch 51: training loss = 0.387607, training acuracy = 88.37%, test loss = 0.377217, test acuracy = 88.82%\n",
      "Epoch 52: training loss = 0.373065, training acuracy = 88.78%, test loss = 0.357372, test acuracy = 89.41%\n",
      "Epoch 53: training loss = 0.365355, training acuracy = 89.2%, test loss = 0.353176, test acuracy = 89.79%\n",
      "Epoch 54: training loss = 0.357512, training acuracy = 89.42%, test loss = 0.343249, test acuracy = 90.13%\n",
      "Epoch 55: training loss = 0.363962, training acuracy = 89.12%, test loss = 0.351002, test acuracy = 89.49%\n",
      "Epoch 56: training loss = 0.366373, training acuracy = 89.16%, test loss = 0.354017, test acuracy = 89.56%\n",
      "Epoch 57: training loss = 0.401361, training acuracy = 87.7%, test loss = 0.388201, test acuracy = 88.23%\n",
      "Epoch 58: training loss = 0.431517, training acuracy = 86.71%, test loss = 0.423165, test acuracy = 86.8%\n",
      "Epoch 59: training loss = 0.524146, training acuracy = 83.54%, test loss = 0.511659, test acuracy = 84.17%\n",
      "Epoch 60: training loss = 0.543529, training acuracy = 82.64%, test loss = 0.542154, test acuracy = 82.62%\n",
      "Epoch 61: training loss = 0.467873, training acuracy = 84.85%, test loss = 0.451047, test acuracy = 85.48%\n",
      "Epoch 62: training loss = 0.376215, training acuracy = 88.9%, test loss = 0.369663, test acuracy = 88.92%\n",
      "Epoch 63: training loss = 0.335805, training acuracy = 90.2%, test loss = 0.32129, test acuracy = 90.96%\n",
      "Epoch 64: training loss = 0.321093, training acuracy = 91.0%, test loss = 0.310413, test acuracy = 91.22%\n",
      "Epoch 65: training loss = 0.313614, training acuracy = 91.02%, test loss = 0.300597, test acuracy = 91.5%\n",
      "Epoch 66: training loss = 0.30888, training acuracy = 91.2%, test loss = 0.297573, test acuracy = 91.68%\n",
      "Epoch 67: training loss = 0.304906, training acuracy = 91.21%, test loss = 0.292532, test acuracy = 91.66%\n",
      "Epoch 68: training loss = 0.3018, training acuracy = 91.33%, test loss = 0.290662, test acuracy = 91.85%\n",
      "Epoch 69: training loss = 0.298871, training acuracy = 91.32%, test loss = 0.28688, test acuracy = 91.77%\n",
      "Epoch 70: training loss = 0.29655, training acuracy = 91.46%, test loss = 0.285738, test acuracy = 91.87%\n",
      "Epoch 71: training loss = 0.294221, training acuracy = 91.41%, test loss = 0.282501, test acuracy = 91.89%\n",
      "Epoch 72: training loss = 0.292593, training acuracy = 91.54%, test loss = 0.282156, test acuracy = 91.92%\n",
      "Epoch 73: training loss = 0.290866, training acuracy = 91.48%, test loss = 0.279354, test acuracy = 91.92%\n",
      "Epoch 74: training loss = 0.290197, training acuracy = 91.56%, test loss = 0.280209, test acuracy = 91.97%\n",
      "Epoch 75: training loss = 0.289234, training acuracy = 91.44%, test loss = 0.277854, test acuracy = 91.97%\n",
      "Epoch 76: training loss = 0.290298, training acuracy = 91.5%, test loss = 0.280847, test acuracy = 91.91%\n",
      "Epoch 77: training loss = 0.290575, training acuracy = 91.32%, test loss = 0.279231, test acuracy = 91.87%\n",
      "Epoch 78: training loss = 0.295255, training acuracy = 91.25%, test loss = 0.286432, test acuracy = 91.51%\n",
      "Epoch 79: training loss = 0.297576, training acuracy = 90.94%, test loss = 0.28616, test acuracy = 91.5%\n",
      "Epoch 80: training loss = 0.310225, training acuracy = 90.57%, test loss = 0.302055, test acuracy = 90.88%\n",
      "Epoch 81: training loss = 0.314886, training acuracy = 90.12%, test loss = 0.303324, test acuracy = 90.68%\n",
      "Epoch 82: training loss = 0.342842, training acuracy = 89.04%, test loss = 0.335077, test acuracy = 89.77%\n",
      "Epoch 83: training loss = 0.339892, training acuracy = 89.14%, test loss = 0.328244, test acuracy = 89.55%\n",
      "Epoch 84: training loss = 0.374505, training acuracy = 87.63%, test loss = 0.366317, test acuracy = 88.38%\n",
      "Epoch 85: training loss = 0.331918, training acuracy = 89.51%, test loss = 0.320664, test acuracy = 90.0%\n",
      "Epoch 86: training loss = 0.325218, training acuracy = 90.0%, test loss = 0.31663, test acuracy = 90.42%\n",
      "Epoch 87: training loss = 0.284601, training acuracy = 91.49%, test loss = 0.274541, test acuracy = 92.04%\n",
      "Epoch 88: training loss = 0.273501, training acuracy = 92.04%, test loss = 0.264859, test acuracy = 92.4%\n",
      "Epoch 89: training loss = 0.264701, training acuracy = 92.26%, test loss = 0.255525, test acuracy = 92.65%\n",
      "Epoch 90: training loss = 0.261764, training acuracy = 92.38%, test loss = 0.253097, test acuracy = 92.71%\n",
      "Epoch 91: training loss = 0.258703, training acuracy = 92.47%, test loss = 0.250069, test acuracy = 92.85%\n",
      "Epoch 92: training loss = 0.256765, training acuracy = 92.51%, test loss = 0.248301, test acuracy = 92.78%\n",
      "Epoch 93: training loss = 0.254581, training acuracy = 92.6%, test loss = 0.24635, test acuracy = 93.02%\n",
      "Epoch 94: training loss = 0.252899, training acuracy = 92.6%, test loss = 0.244679, test acuracy = 92.89%\n",
      "Epoch 95: training loss = 0.251043, training acuracy = 92.67%, test loss = 0.243149, test acuracy = 93.14%\n",
      "Epoch 96: training loss = 0.249514, training acuracy = 92.7%, test loss = 0.241532, test acuracy = 92.96%\n",
      "Epoch 97: training loss = 0.247855, training acuracy = 92.77%, test loss = 0.240266, test acuracy = 93.23%\n",
      "Epoch 98: training loss = 0.246446, training acuracy = 92.79%, test loss = 0.238691, test acuracy = 93.0%\n",
      "Epoch 99: training loss = 0.244901, training acuracy = 92.86%, test loss = 0.237593, test acuracy = 93.27%\n",
      "Epoch 100: training loss = 0.243595, training acuracy = 92.88%, test loss = 0.23605, test acuracy = 93.04%\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.5 #주어진 learning_rate 값을 이용하였다.\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  if i == 0:\n",
    "    params = foward_pass(x_train, params)\n",
    "    \n",
    "  grads = backward_pass(x_train, y_train, params)\n",
    "\n",
    "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "  params[\"W3\"] -= learning_rate * grads[\"dW3\"]\n",
    "  params[\"b3\"] -= learning_rate * grads[\"db3\"]\n",
    "\n",
    "  params = foward_pass(x_train, params)\n",
    "  train_loss = compute_loss(y_train, params[\"A3\"])\n",
    "  train_acc = compute_accuracy(y_train, params[\"A3\"])\n",
    "\n",
    "  params_test = foward_pass_test(x_test, params)\n",
    "  test_loss = compute_loss(y_test, params_test[\"A3\"])\n",
    "  test_acc = compute_accuracy(y_test, params_test[\"A3\"])\n",
    "\n",
    "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, test acuracy = {}%\"\n",
    "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WaqqRzF73oBu"
   },
   "source": [
    "## Assignment 2-4\n",
    "Lecture 2의 2. backpropagatin with numpy 부분의 성능을 지금까지 배운 지식을 바탕으로 향상시켜보세요\n",
    "\n",
    "- Hint : Activation function, hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1632620769319,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "k6b82DZG6W3j"
   },
   "outputs": [],
   "source": [
    "# Assignment 2-4 구현은 여기서 ()\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 22654,
     "status": "ok",
     "timestamp": 1632623653029,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "C_g7CCnITq6Y"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "mnist = sklearn.datasets.fetch_openml('mnist_784', data_home=\"mnist_784\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1632623654824,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "A1AGmKjyTq3L"
   },
   "outputs": [],
   "source": [
    "\n",
    "num_train = 60000\n",
    "num_class = 10\n",
    "\n",
    "x_train = np.float32(mnist.data[:num_train]).T\n",
    "y_train_index = np.int32(mnist.target[:num_train]).T\n",
    "x_test = np.float32(mnist.data[num_train:]).T\n",
    "y_test_index = np.int32(mnist.target[num_train:]).T\n",
    "\n",
    "x_train /= 255 #8비트 범위이기 때문에 모든 값을 0~1사이로 바꿀 수 있다.\n",
    "x_test /= 255\n",
    "x_size = x_train.shape[0]\n",
    "\n",
    "y_train = np.zeros((num_class, y_train_index.shape[0]))\n",
    "for idx in range(y_train_index.shape[0]):\n",
    "  y_train[y_train_index[idx], idx] = 1\n",
    "\n",
    "y_test = np.zeros((num_class, y_test_index.shape[0]))\n",
    "for idx in range(y_test_index.shape[0]):\n",
    "  y_test[y_test_index[idx], idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632623656179,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "cn-coh5QTqxm"
   },
   "outputs": [],
   "source": [
    "hidden_size = 200\n",
    "# hidden_unit_size가 높을 수록 더 매끄러운 결정경계를 형성하게 된다. 다만 이 값이 너무 크면 모델이 무거워져 속도가 느려지므로 적절한 값을 찾아야 한다.\n",
    "\n",
    "\n",
    "params = {\"W1\": np.random.randn(hidden_size, x_size) * np.sqrt(1/ x_size),\n",
    "          \"b1\": np.zeros((hidden_size, 1)) * np.sqrt(1/ x_size),\n",
    "          \"W2\": np.random.randn(num_class, hidden_size) * np.sqrt(1/ hidden_size),\n",
    "          \"b2\": np.zeros((num_class, 1)) * np.sqrt(1/ hidden_size)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632623657561,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "i2gK6BzrUBKs"
   },
   "outputs": [],
   "source": [
    "def relu(x): #예제와 다르게 activation function으로 relu함수를 사용할 것이다. 이것이 모델의 성능을 높이는데 가장 큰 영향을 주는 것으로 보인다.\n",
    "  return np.maximum(0,x)\n",
    "\n",
    "def d_relu(x):\n",
    "  x[x<=0]=0\n",
    "  x[x>0]=1\n",
    "  return x\n",
    "\n",
    "def softmax(x):\n",
    "  exp = np.exp(x)\n",
    "  return exp/np.sum(exp, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632623658160,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "mG_k0RozTqu6"
   },
   "outputs": [],
   "source": [
    "def compute_loss(y_true, y_pred):\n",
    "  # loss calculation\n",
    "\n",
    "  num_sample = y_true.shape[1]\n",
    "  Li = -1 * np.sum(y_true * np.log(y_pred))\n",
    "  \n",
    "  return Li/num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632623658160,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "EyezQFoGTqsu"
   },
   "outputs": [],
   "source": [
    "def foward_pass(x, params): #기존 예제에 있던 2-layer를 이용하였다.\n",
    "  \n",
    "  params[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "  params[\"A1\"] = relu(params[\"S1\"]) #sigmoid대신 relu를 사용한다.\n",
    "  params[\"S2\"] = np.dot(params[\"W2\"], params[\"A1\"]) + params[\"b2\"]\n",
    "  params[\"A2\"] = softmax(params[\"S2\"])\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632623658161,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "U4o1bpagTqqB"
   },
   "outputs": [],
   "source": [
    "def foward_pass_test(x, params): #바로 위의 foward_pass 코드와 완전히 동일하다.\n",
    "\n",
    "  params_test = {}\n",
    "  \n",
    "  params_test[\"S1\"] = np.dot(params[\"W1\"], x) + params[\"b1\"]\n",
    "  params_test[\"A1\"] = relu(params_test[\"S1\"])\n",
    "  params_test[\"S2\"] = np.dot(params[\"W2\"], params_test[\"A1\"]) + params[\"b2\"]\n",
    "  params_test[\"A2\"] = softmax(params_test[\"S2\"])\n",
    "\n",
    "  return params_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1632623658625,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "4CdPAOlmTqmq"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred): #정확도를 계산하는 코드이다.\n",
    "  y_true_idx = np.argmax(y_true, axis = 0)\n",
    "  y_pred_idx = np.argmax(y_pred, axis = 0)\n",
    "  num_correct = np.sum(y_true_idx==y_pred_idx)\n",
    "\n",
    "  accuracy = num_correct / y_true.shape[1] * 100\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1632623658625,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "wNq_XdWnTqhk"
   },
   "outputs": [],
   "source": [
    "def backward_pass(x, y_true, params):\n",
    "\n",
    "  dS2 = params[\"A2\"] - y_true\n",
    "\n",
    "  grads = {}\n",
    "\n",
    "  grads[\"dW2\"] =  np.dot(dS2, params[\"A1\"].T)/x.shape[1]\n",
    "  grads[\"db2\"] =  np.sum(dS2, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  dA1 = np.dot(params[\"W2\"].T, dS2)\n",
    "  dS1 = dA1 * d_relu(params[\"S1\"]) #backpropagation과정에서도 역시 sigmoid의 derivative대신 relu의 derivative를 사용하였다.\n",
    "\n",
    "  grads[\"dW1\"] = np.dot(dS1, x.T)/x.shape[1]\n",
    "  grads[\"db1\"] = np.sum(dS1, axis=1, keepdims=True)/x.shape[1]\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300502,
     "status": "ok",
     "timestamp": 1632623959510,
     "user": {
      "displayName": "규",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10356754072138524849"
     },
     "user_tz": -540
    },
    "id": "8kQpsy9mTqPj",
    "outputId": "52752659-6cb3-4ba9-b972-2f552601d003"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: training loss = 1.814427, training acuracy = 57.72%, test loss = 1.80297, test acuracy = 58.69%\n",
      "Epoch 2: training loss = 1.597902, training acuracy = 47.55%, test loss = 1.589587, test acuracy = 47.58%\n",
      "Epoch 3: training loss = 3.292512, training acuracy = 30.36%, test loss = 3.294209, test acuracy = 30.66%\n",
      "Epoch 4: training loss = 2.323113, training acuracy = 42.13%, test loss = 2.322288, test acuracy = 42.85%\n",
      "Epoch 5: training loss = 1.696913, training acuracy = 44.2%, test loss = 1.693592, test acuracy = 43.97%\n",
      "Epoch 6: training loss = 1.408135, training acuracy = 58.57%, test loss = 1.388817, test acuracy = 58.59%\n",
      "Epoch 7: training loss = 1.555658, training acuracy = 44.83%, test loss = 1.55701, test acuracy = 45.01%\n",
      "Epoch 8: training loss = 2.244915, training acuracy = 46.86%, test loss = 2.240607, test acuracy = 47.34%\n",
      "Epoch 9: training loss = 2.293527, training acuracy = 33.25%, test loss = 2.312193, test acuracy = 32.38%\n",
      "Epoch 10: training loss = 1.760777, training acuracy = 40.84%, test loss = 1.72615, test acuracy = 41.4%\n",
      "Epoch 11: training loss = 1.4305, training acuracy = 52.2%, test loss = 1.422919, test acuracy = 52.41%\n",
      "Epoch 12: training loss = 1.243842, training acuracy = 55.42%, test loss = 1.229967, test acuracy = 55.32%\n",
      "Epoch 13: training loss = 1.389534, training acuracy = 49.46%, test loss = 1.385371, test acuracy = 49.2%\n",
      "Epoch 14: training loss = 1.412577, training acuracy = 49.96%, test loss = 1.403188, test acuracy = 49.61%\n",
      "Epoch 15: training loss = 1.506369, training acuracy = 56.29%, test loss = 1.493565, test acuracy = 57.33%\n",
      "Epoch 16: training loss = 1.523595, training acuracy = 44.19%, test loss = 1.512549, test acuracy = 44.23%\n",
      "Epoch 17: training loss = 1.462086, training acuracy = 52.54%, test loss = 1.474904, test acuracy = 52.58%\n",
      "Epoch 18: training loss = 1.627508, training acuracy = 55.81%, test loss = 1.646612, test acuracy = 55.94%\n",
      "Epoch 19: training loss = 1.251983, training acuracy = 61.82%, test loss = 1.263717, test acuracy = 61.47%\n",
      "Epoch 20: training loss = 1.096993, training acuracy = 62.09%, test loss = 1.082354, test acuracy = 62.69%\n",
      "Epoch 21: training loss = 0.91166, training acuracy = 68.97%, test loss = 0.912152, test acuracy = 68.78%\n",
      "Epoch 22: training loss = 0.730466, training acuracy = 74.58%, test loss = 0.710826, test acuracy = 75.28%\n",
      "Epoch 23: training loss = 0.734424, training acuracy = 73.57%, test loss = 0.73051, test acuracy = 73.38%\n",
      "Epoch 24: training loss = 0.772803, training acuracy = 72.22%, test loss = 0.762397, test acuracy = 72.68%\n",
      "Epoch 25: training loss = 0.878127, training acuracy = 69.53%, test loss = 0.864269, test acuracy = 69.83%\n",
      "Epoch 26: training loss = 0.830748, training acuracy = 70.39%, test loss = 0.830621, test acuracy = 70.32%\n",
      "Epoch 27: training loss = 0.776905, training acuracy = 72.62%, test loss = 0.763666, test acuracy = 72.65%\n",
      "Epoch 28: training loss = 0.784606, training acuracy = 72.71%, test loss = 0.774365, test acuracy = 72.66%\n",
      "Epoch 29: training loss = 0.614337, training acuracy = 80.0%, test loss = 0.603767, test acuracy = 79.98%\n",
      "Epoch 30: training loss = 0.513356, training acuracy = 85.12%, test loss = 0.497237, test acuracy = 85.58%\n",
      "Epoch 31: training loss = 0.457267, training acuracy = 85.83%, test loss = 0.444848, test acuracy = 86.14%\n",
      "Epoch 32: training loss = 0.449544, training acuracy = 86.61%, test loss = 0.433584, test acuracy = 86.86%\n",
      "Epoch 33: training loss = 0.436226, training acuracy = 86.04%, test loss = 0.425708, test acuracy = 86.38%\n",
      "Epoch 34: training loss = 0.468782, training acuracy = 84.94%, test loss = 0.455146, test acuracy = 85.19%\n",
      "Epoch 35: training loss = 0.524863, training acuracy = 80.97%, test loss = 0.520556, test acuracy = 81.44%\n",
      "Epoch 36: training loss = 0.634804, training acuracy = 77.4%, test loss = 0.627212, test acuracy = 77.04%\n",
      "Epoch 37: training loss = 0.649492, training acuracy = 78.41%, test loss = 0.64455, test acuracy = 78.74%\n",
      "Epoch 38: training loss = 0.540045, training acuracy = 80.64%, test loss = 0.531375, test acuracy = 80.77%\n",
      "Epoch 39: training loss = 0.489525, training acuracy = 85.0%, test loss = 0.479917, test acuracy = 85.42%\n",
      "Epoch 40: training loss = 0.381469, training acuracy = 89.17%, test loss = 0.368952, test acuracy = 89.54%\n",
      "Epoch 41: training loss = 0.363036, training acuracy = 89.58%, test loss = 0.350614, test acuracy = 90.22%\n",
      "Epoch 42: training loss = 0.354104, training acuracy = 89.41%, test loss = 0.340806, test acuracy = 89.94%\n",
      "Epoch 43: training loss = 0.358673, training acuracy = 89.27%, test loss = 0.347755, test acuracy = 89.74%\n",
      "Epoch 44: training loss = 0.371366, training acuracy = 88.3%, test loss = 0.357498, test acuracy = 88.79%\n",
      "Epoch 45: training loss = 0.438307, training acuracy = 85.98%, test loss = 0.434636, test acuracy = 86.3%\n",
      "Epoch 46: training loss = 0.469019, training acuracy = 84.5%, test loss = 0.457404, test acuracy = 84.78%\n",
      "Epoch 47: training loss = 0.48929, training acuracy = 84.4%, test loss = 0.487578, test acuracy = 84.5%\n",
      "Epoch 48: training loss = 0.354335, training acuracy = 89.87%, test loss = 0.34374, test acuracy = 90.35%\n",
      "Epoch 49: training loss = 0.333492, training acuracy = 90.16%, test loss = 0.324009, test acuracy = 90.39%\n",
      "Epoch 50: training loss = 0.323045, training acuracy = 90.57%, test loss = 0.312342, test acuracy = 91.13%\n",
      "Epoch 51: training loss = 0.314159, training acuracy = 90.68%, test loss = 0.304088, test acuracy = 90.8%\n",
      "Epoch 52: training loss = 0.308132, training acuracy = 90.94%, test loss = 0.297071, test acuracy = 91.42%\n",
      "Epoch 53: training loss = 0.302628, training acuracy = 91.0%, test loss = 0.292228, test acuracy = 91.19%\n",
      "Epoch 54: training loss = 0.299212, training acuracy = 91.14%, test loss = 0.288111, test acuracy = 91.68%\n",
      "Epoch 55: training loss = 0.295296, training acuracy = 91.2%, test loss = 0.284839, test acuracy = 91.47%\n",
      "Epoch 56: training loss = 0.294735, training acuracy = 91.21%, test loss = 0.283799, test acuracy = 91.77%\n",
      "Epoch 57: training loss = 0.292381, training acuracy = 91.28%, test loss = 0.282019, test acuracy = 91.52%\n",
      "Epoch 58: training loss = 0.297766, training acuracy = 91.03%, test loss = 0.286979, test acuracy = 91.6%\n",
      "Epoch 59: training loss = 0.298696, training acuracy = 91.16%, test loss = 0.288397, test acuracy = 91.45%\n",
      "Epoch 60: training loss = 0.321173, training acuracy = 90.08%, test loss = 0.310444, test acuracy = 90.75%\n",
      "Epoch 61: training loss = 0.334557, training acuracy = 89.86%, test loss = 0.323661, test acuracy = 90.24%\n",
      "Epoch 62: training loss = 0.407814, training acuracy = 86.86%, test loss = 0.397687, test acuracy = 87.3%\n",
      "Epoch 63: training loss = 0.482126, training acuracy = 84.15%, test loss = 0.471868, test acuracy = 84.42%\n",
      "Epoch 64: training loss = 0.459645, training acuracy = 85.37%, test loss = 0.453339, test acuracy = 85.62%\n",
      "Epoch 65: training loss = 0.430253, training acuracy = 86.22%, test loss = 0.415893, test acuracy = 87.01%\n",
      "Epoch 66: training loss = 0.314098, training acuracy = 90.2%, test loss = 0.307057, test acuracy = 90.4%\n",
      "Epoch 67: training loss = 0.274803, training acuracy = 92.16%, test loss = 0.264122, test acuracy = 92.56%\n",
      "Epoch 68: training loss = 0.263797, training acuracy = 92.2%, test loss = 0.254667, test acuracy = 92.48%\n",
      "Epoch 69: training loss = 0.258651, training acuracy = 92.44%, test loss = 0.249274, test acuracy = 92.77%\n",
      "Epoch 70: training loss = 0.25503, training acuracy = 92.47%, test loss = 0.245863, test acuracy = 92.81%\n",
      "Epoch 71: training loss = 0.251996, training acuracy = 92.58%, test loss = 0.242995, test acuracy = 92.96%\n",
      "Epoch 72: training loss = 0.249351, training acuracy = 92.64%, test loss = 0.240466, test acuracy = 93.01%\n",
      "Epoch 73: training loss = 0.246952, training acuracy = 92.71%, test loss = 0.238333, test acuracy = 93.1%\n",
      "Epoch 74: training loss = 0.244724, training acuracy = 92.78%, test loss = 0.236119, test acuracy = 93.12%\n",
      "Epoch 75: training loss = 0.24263, training acuracy = 92.84%, test loss = 0.234333, test acuracy = 93.18%\n",
      "Epoch 76: training loss = 0.240639, training acuracy = 92.9%, test loss = 0.232277, test acuracy = 93.22%\n",
      "Epoch 77: training loss = 0.238734, training acuracy = 92.97%, test loss = 0.230736, test acuracy = 93.27%\n",
      "Epoch 78: training loss = 0.236911, training acuracy = 93.0%, test loss = 0.22878, test acuracy = 93.26%\n",
      "Epoch 79: training loss = 0.235185, training acuracy = 93.08%, test loss = 0.227486, test acuracy = 93.34%\n",
      "Epoch 80: training loss = 0.233547, training acuracy = 93.11%, test loss = 0.225644, test acuracy = 93.4%\n",
      "Epoch 81: training loss = 0.232034, training acuracy = 93.17%, test loss = 0.224677, test acuracy = 93.38%\n",
      "Epoch 82: training loss = 0.230644, training acuracy = 93.18%, test loss = 0.222944, test acuracy = 93.46%\n",
      "Epoch 83: training loss = 0.229472, training acuracy = 93.24%, test loss = 0.222494, test acuracy = 93.4%\n",
      "Epoch 84: training loss = 0.228548, training acuracy = 93.19%, test loss = 0.221007, test acuracy = 93.5%\n",
      "Epoch 85: training loss = 0.228154, training acuracy = 93.29%, test loss = 0.221633, test acuracy = 93.41%\n",
      "Epoch 86: training loss = 0.228322, training acuracy = 93.14%, test loss = 0.220908, test acuracy = 93.52%\n",
      "Epoch 87: training loss = 0.229992, training acuracy = 93.2%, test loss = 0.224113, test acuracy = 93.33%\n",
      "Epoch 88: training loss = 0.233104, training acuracy = 92.89%, test loss = 0.225842, test acuracy = 93.17%\n",
      "Epoch 89: training loss = 0.241225, training acuracy = 92.81%, test loss = 0.236444, test acuracy = 92.8%\n",
      "Epoch 90: training loss = 0.252692, training acuracy = 92.04%, test loss = 0.245885, test acuracy = 92.22%\n",
      "Epoch 91: training loss = 0.283202, training acuracy = 91.29%, test loss = 0.280933, test acuracy = 91.38%\n",
      "Epoch 92: training loss = 0.316871, training acuracy = 89.47%, test loss = 0.312372, test acuracy = 89.29%\n",
      "Epoch 93: training loss = 0.417503, training acuracy = 86.34%, test loss = 0.423328, test acuracy = 85.88%\n",
      "Epoch 94: training loss = 0.397909, training acuracy = 86.14%, test loss = 0.399666, test acuracy = 85.98%\n",
      "Epoch 95: training loss = 0.408832, training acuracy = 86.34%, test loss = 0.413379, test acuracy = 86.15%\n",
      "Epoch 96: training loss = 0.263605, training acuracy = 91.84%, test loss = 0.257711, test acuracy = 91.93%\n",
      "Epoch 97: training loss = 0.234386, training acuracy = 93.28%, test loss = 0.228464, test acuracy = 93.29%\n",
      "Epoch 98: training loss = 0.224709, training acuracy = 93.35%, test loss = 0.217962, test acuracy = 93.5%\n",
      "Epoch 99: training loss = 0.220259, training acuracy = 93.56%, test loss = 0.214325, test acuracy = 93.65%\n",
      "Epoch 100: training loss = 0.216959, training acuracy = 93.59%, test loss = 0.210805, test acuracy = 93.75%\n",
      "Epoch 101: training loss = 0.214256, training acuracy = 93.68%, test loss = 0.208639, test acuracy = 93.82%\n",
      "Epoch 102: training loss = 0.212059, training acuracy = 93.69%, test loss = 0.20626, test acuracy = 93.86%\n",
      "Epoch 103: training loss = 0.210041, training acuracy = 93.78%, test loss = 0.20468, test acuracy = 93.91%\n",
      "Epoch 104: training loss = 0.208342, training acuracy = 93.8%, test loss = 0.20278, test acuracy = 93.92%\n",
      "Epoch 105: training loss = 0.20671, training acuracy = 93.88%, test loss = 0.201571, test acuracy = 94.03%\n",
      "Epoch 106: training loss = 0.205311, training acuracy = 93.9%, test loss = 0.199936, test acuracy = 94.04%\n",
      "Epoch 107: training loss = 0.203923, training acuracy = 93.95%, test loss = 0.198995, test acuracy = 94.08%\n",
      "Epoch 108: training loss = 0.202731, training acuracy = 93.98%, test loss = 0.197537, test acuracy = 94.11%\n",
      "Epoch 109: training loss = 0.201545, training acuracy = 94.02%, test loss = 0.19683, test acuracy = 94.2%\n",
      "Epoch 110: training loss = 0.200529, training acuracy = 94.05%, test loss = 0.195515, test acuracy = 94.22%\n",
      "Epoch 111: training loss = 0.199513, training acuracy = 94.08%, test loss = 0.194984, test acuracy = 94.27%\n",
      "Epoch 112: training loss = 0.198692, training acuracy = 94.12%, test loss = 0.193846, test acuracy = 94.25%\n",
      "Epoch 113: training loss = 0.197853, training acuracy = 94.11%, test loss = 0.193522, test acuracy = 94.27%\n",
      "Epoch 114: training loss = 0.197241, training acuracy = 94.18%, test loss = 0.192561, test acuracy = 94.24%\n",
      "Epoch 115: training loss = 0.196612, training acuracy = 94.16%, test loss = 0.192484, test acuracy = 94.3%\n",
      "Epoch 116: training loss = 0.196245, training acuracy = 94.22%, test loss = 0.191735, test acuracy = 94.15%\n",
      "Epoch 117: training loss = 0.195853, training acuracy = 94.19%, test loss = 0.191949, test acuracy = 94.26%\n",
      "Epoch 118: training loss = 0.195786, training acuracy = 94.2%, test loss = 0.191437, test acuracy = 94.17%\n",
      "Epoch 119: training loss = 0.195615, training acuracy = 94.21%, test loss = 0.191939, test acuracy = 94.23%\n",
      "Epoch 120: training loss = 0.195969, training acuracy = 94.19%, test loss = 0.191792, test acuracy = 94.15%\n",
      "Epoch 121: training loss = 0.196057, training acuracy = 94.18%, test loss = 0.192619, test acuracy = 94.2%\n",
      "Epoch 122: training loss = 0.19682, training acuracy = 94.18%, test loss = 0.192827, test acuracy = 94.17%\n",
      "Epoch 123: training loss = 0.196964, training acuracy = 94.15%, test loss = 0.193763, test acuracy = 94.06%\n",
      "Epoch 124: training loss = 0.198068, training acuracy = 94.12%, test loss = 0.19428, test acuracy = 94.01%\n",
      "Epoch 125: training loss = 0.197948, training acuracy = 94.08%, test loss = 0.194963, test acuracy = 93.93%\n",
      "Epoch 126: training loss = 0.198943, training acuracy = 94.07%, test loss = 0.195349, test acuracy = 93.94%\n",
      "Epoch 127: training loss = 0.197933, training acuracy = 94.08%, test loss = 0.195159, test acuracy = 93.88%\n",
      "Epoch 128: training loss = 0.19822, training acuracy = 94.08%, test loss = 0.194801, test acuracy = 93.96%\n",
      "Epoch 129: training loss = 0.196057, training acuracy = 94.16%, test loss = 0.193444, test acuracy = 93.93%\n",
      "Epoch 130: training loss = 0.195204, training acuracy = 94.21%, test loss = 0.191962, test acuracy = 94.05%\n",
      "Epoch 131: training loss = 0.192043, training acuracy = 94.28%, test loss = 0.189546, test acuracy = 94.15%\n",
      "Epoch 132: training loss = 0.190245, training acuracy = 94.37%, test loss = 0.187161, test acuracy = 94.28%\n",
      "Epoch 133: training loss = 0.186898, training acuracy = 94.48%, test loss = 0.18449, test acuracy = 94.51%\n",
      "Epoch 134: training loss = 0.184895, training acuracy = 94.56%, test loss = 0.181965, test acuracy = 94.48%\n",
      "Epoch 135: training loss = 0.182208, training acuracy = 94.64%, test loss = 0.179874, test acuracy = 94.68%\n",
      "Epoch 136: training loss = 0.180456, training acuracy = 94.69%, test loss = 0.177693, test acuracy = 94.69%\n",
      "Epoch 137: training loss = 0.1785, training acuracy = 94.78%, test loss = 0.176271, test acuracy = 94.82%\n",
      "Epoch 138: training loss = 0.177113, training acuracy = 94.81%, test loss = 0.174505, test acuracy = 94.85%\n",
      "Epoch 139: training loss = 0.175681, training acuracy = 94.86%, test loss = 0.173573, test acuracy = 94.93%\n",
      "Epoch 140: training loss = 0.174571, training acuracy = 94.91%, test loss = 0.172136, test acuracy = 94.94%\n",
      "Epoch 141: training loss = 0.173438, training acuracy = 94.92%, test loss = 0.171474, test acuracy = 94.94%\n",
      "Epoch 142: training loss = 0.172497, training acuracy = 95.0%, test loss = 0.170238, test acuracy = 95.01%\n",
      "Epoch 143: training loss = 0.171552, training acuracy = 94.99%, test loss = 0.169733, test acuracy = 95.01%\n",
      "Epoch 144: training loss = 0.170713, training acuracy = 95.06%, test loss = 0.168631, test acuracy = 95.05%\n",
      "Epoch 145: training loss = 0.169879, training acuracy = 95.04%, test loss = 0.168204, test acuracy = 95.07%\n",
      "Epoch 146: training loss = 0.169122, training acuracy = 95.11%, test loss = 0.167217, test acuracy = 95.07%\n",
      "Epoch 147: training loss = 0.168361, training acuracy = 95.09%, test loss = 0.166834, test acuracy = 95.15%\n",
      "Epoch 148: training loss = 0.167657, training acuracy = 95.14%, test loss = 0.16592, test acuracy = 95.09%\n",
      "Epoch 149: training loss = 0.16695, training acuracy = 95.12%, test loss = 0.165564, test acuracy = 95.16%\n",
      "Epoch 150: training loss = 0.166281, training acuracy = 95.16%, test loss = 0.164708, test acuracy = 95.16%\n",
      "Epoch 151: training loss = 0.165605, training acuracy = 95.16%, test loss = 0.164361, test acuracy = 95.19%\n",
      "Epoch 152: training loss = 0.164964, training acuracy = 95.2%, test loss = 0.163554, test acuracy = 95.18%\n",
      "Epoch 153: training loss = 0.164319, training acuracy = 95.19%, test loss = 0.163218, test acuracy = 95.2%\n",
      "Epoch 154: training loss = 0.163705, training acuracy = 95.24%, test loss = 0.162451, test acuracy = 95.2%\n",
      "Epoch 155: training loss = 0.163085, training acuracy = 95.23%, test loss = 0.162136, test acuracy = 95.24%\n",
      "Epoch 156: training loss = 0.162496, training acuracy = 95.28%, test loss = 0.1614, test acuracy = 95.23%\n",
      "Epoch 157: training loss = 0.161892, training acuracy = 95.27%, test loss = 0.161092, test acuracy = 95.25%\n",
      "Epoch 158: training loss = 0.161325, training acuracy = 95.3%, test loss = 0.160391, test acuracy = 95.28%\n",
      "Epoch 159: training loss = 0.160736, training acuracy = 95.31%, test loss = 0.160086, test acuracy = 95.28%\n",
      "Epoch 160: training loss = 0.160192, training acuracy = 95.33%, test loss = 0.159421, test acuracy = 95.34%\n",
      "Epoch 161: training loss = 0.159628, training acuracy = 95.36%, test loss = 0.159132, test acuracy = 95.31%\n",
      "Epoch 162: training loss = 0.159101, training acuracy = 95.36%, test loss = 0.158502, test acuracy = 95.38%\n",
      "Epoch 163: training loss = 0.158555, training acuracy = 95.38%, test loss = 0.158224, test acuracy = 95.32%\n",
      "Epoch 164: training loss = 0.15804, training acuracy = 95.39%, test loss = 0.157603, test acuracy = 95.42%\n",
      "Epoch 165: training loss = 0.157515, training acuracy = 95.41%, test loss = 0.15735, test acuracy = 95.32%\n",
      "Epoch 166: training loss = 0.157032, training acuracy = 95.42%, test loss = 0.156756, test acuracy = 95.4%\n",
      "Epoch 167: training loss = 0.156522, training acuracy = 95.46%, test loss = 0.156524, test acuracy = 95.35%\n",
      "Epoch 168: training loss = 0.156073, training acuracy = 95.46%, test loss = 0.15596, test acuracy = 95.41%\n",
      "Epoch 169: training loss = 0.155581, training acuracy = 95.48%, test loss = 0.155753, test acuracy = 95.37%\n",
      "Epoch 170: training loss = 0.155163, training acuracy = 95.5%, test loss = 0.155213, test acuracy = 95.42%\n",
      "Epoch 171: training loss = 0.154695, training acuracy = 95.51%, test loss = 0.155041, test acuracy = 95.37%\n",
      "Epoch 172: training loss = 0.154324, training acuracy = 95.52%, test loss = 0.154539, test acuracy = 95.43%\n",
      "Epoch 173: training loss = 0.153891, training acuracy = 95.53%, test loss = 0.154415, test acuracy = 95.38%\n",
      "Epoch 174: training loss = 0.15356, training acuracy = 95.54%, test loss = 0.153938, test acuracy = 95.44%\n",
      "Epoch 175: training loss = 0.153145, training acuracy = 95.53%, test loss = 0.153849, test acuracy = 95.37%\n",
      "Epoch 176: training loss = 0.152864, training acuracy = 95.56%, test loss = 0.153395, test acuracy = 95.42%\n",
      "Epoch 177: training loss = 0.152468, training acuracy = 95.55%, test loss = 0.153348, test acuracy = 95.38%\n",
      "Epoch 178: training loss = 0.15224, training acuracy = 95.57%, test loss = 0.15293, test acuracy = 95.42%\n",
      "Epoch 179: training loss = 0.151849, training acuracy = 95.57%, test loss = 0.1529, test acuracy = 95.39%\n",
      "Epoch 180: training loss = 0.151652, training acuracy = 95.6%, test loss = 0.1525, test acuracy = 95.45%\n",
      "Epoch 181: training loss = 0.151275, training acuracy = 95.58%, test loss = 0.152495, test acuracy = 95.38%\n",
      "Epoch 182: training loss = 0.151128, training acuracy = 95.6%, test loss = 0.152131, test acuracy = 95.45%\n",
      "Epoch 183: training loss = 0.150741, training acuracy = 95.59%, test loss = 0.15213, test acuracy = 95.37%\n",
      "Epoch 184: training loss = 0.150635, training acuracy = 95.59%, test loss = 0.151797, test acuracy = 95.48%\n",
      "Epoch 185: training loss = 0.150217, training acuracy = 95.6%, test loss = 0.151773, test acuracy = 95.38%\n",
      "Epoch 186: training loss = 0.150106, training acuracy = 95.6%, test loss = 0.151413, test acuracy = 95.5%\n",
      "Epoch 187: training loss = 0.149608, training acuracy = 95.61%, test loss = 0.151323, test acuracy = 95.39%\n",
      "Epoch 188: training loss = 0.149497, training acuracy = 95.6%, test loss = 0.150954, test acuracy = 95.48%\n",
      "Epoch 189: training loss = 0.14892, training acuracy = 95.62%, test loss = 0.150799, test acuracy = 95.35%\n",
      "Epoch 190: training loss = 0.148768, training acuracy = 95.63%, test loss = 0.150373, test acuracy = 95.48%\n",
      "Epoch 191: training loss = 0.148113, training acuracy = 95.66%, test loss = 0.150146, test acuracy = 95.36%\n",
      "Epoch 192: training loss = 0.147889, training acuracy = 95.66%, test loss = 0.149646, test acuracy = 95.5%\n",
      "Epoch 193: training loss = 0.147156, training acuracy = 95.69%, test loss = 0.149338, test acuracy = 95.43%\n",
      "Epoch 194: training loss = 0.146845, training acuracy = 95.69%, test loss = 0.148749, test acuracy = 95.48%\n",
      "Epoch 195: training loss = 0.146054, training acuracy = 95.74%, test loss = 0.148378, test acuracy = 95.44%\n",
      "Epoch 196: training loss = 0.145644, training acuracy = 95.74%, test loss = 0.147698, test acuracy = 95.52%\n",
      "Epoch 197: training loss = 0.144817, training acuracy = 95.78%, test loss = 0.147275, test acuracy = 95.45%\n",
      "Epoch 198: training loss = 0.144341, training acuracy = 95.79%, test loss = 0.146549, test acuracy = 95.53%\n",
      "Epoch 199: training loss = 0.143508, training acuracy = 95.83%, test loss = 0.146098, test acuracy = 95.51%\n",
      "Epoch 200: training loss = 0.142986, training acuracy = 95.84%, test loss = 0.145344, test acuracy = 95.57%\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "learning_rate = 1.3 # learning_rate가 1.5 이상인 경우 epoch 100회까지도 학습이 잘 되지 않는다는 것을 확인하였고 1.3일 경우에는 학습이 더 잘 되는 것을 확인하였다.\n",
    "\n",
    "for i in range(epochs):\n",
    "\n",
    "  if i == 0:\n",
    "    params = foward_pass(x_train, params)\n",
    "    \n",
    "  grads = backward_pass(x_train, y_train, params)\n",
    "\n",
    "  params[\"W1\"] -= learning_rate * grads[\"dW1\"]\n",
    "  params[\"b1\"] -= learning_rate * grads[\"db1\"]\n",
    "  params[\"W2\"] -= learning_rate * grads[\"dW2\"]\n",
    "  params[\"b2\"] -= learning_rate * grads[\"db2\"]\n",
    "\n",
    "  params = foward_pass(x_train, params)\n",
    "  train_loss = compute_loss(y_train, params[\"A2\"])\n",
    "  train_acc = compute_accuracy(y_train, params[\"A2\"])\n",
    "\n",
    "  params_test = foward_pass_test(x_test, params)\n",
    "  test_loss = compute_loss(y_test, params_test[\"A2\"])\n",
    "  test_acc = compute_accuracy(y_test, params_test[\"A2\"])\n",
    "\n",
    "  print(\"Epoch {}: training loss = {}, training acuracy = {}%, test loss = {}, test acuracy = {}%\"\n",
    "  .format(i + 1, np.round(train_loss, 6), np.round(train_acc, 2), np.round(test_loss, 6), np.round(test_acc, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pboMIBQq7onH"
   },
   "source": [
    "**무엇을 보완하였고, 왜 보완되었는지에 대한 자유 서술 (아래에)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ux7mPf6E78d4"
   },
   "outputs": [],
   "source": [
    "#아래 텍스트 칸에 설명하겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7PCMNcjOBsg"
   },
   "source": [
    "먼저 Activation function을 sigmoid 에서 relu로 바꿔주었다. sigmoid함수의 경우 입력으로 들어온 모든 값에 대해 0과 1사이의 출력을 내보내 gradient vanishing과 같은 현상이 일어나고 수렴하는 뉴런의 기울기 값이 0이 된다. 이런 문제가 없는 relu함수로 대체하니 정확도가 90%정도로 오르는 것을 확인할 수 있었다.\n",
    "\n",
    "hidden_unit은 hidden_layer의 노드의 수를 정하는 하이퍼파라미터이다. hidden_unit의 개수가 많을 수록 데이터셋은 더 비선형적이고 매끄러운 경계를 만들 수 있지만 대신 모델의 성능이 무거워진다. hidden_unit의 개수가 적다면 매끄러운 경계를 만들기는 비교적 어렵지만, 이를 적절한 수준으로 줄이는 것은 모델을 더 가볍고 빠르게 만들면서 모델의 정확도는 비슷한 수준으로 만들어줄 수 있다.\n",
    "hidden_unit을 경험적으로 100~200사이의 값으로 설정하는 경우가 많다고 들었다. 다른 조건을 유지하고 이 값을 64, 100, 200, 256, 400 등으로 실험을 해보았다. 64에서 200까지 올라갈 때는 각 단계마다 어느정도 수준의 성능 향상을 보이며 속도도 그리 느리지 않았지만 200일때와 400일때를 비교해보니 정확도는 0.2%정도 차이가 나는 반면 모델 속도는 2배이상 차이가 났다. 이런 경험에 의해 적절한 hidden_unit값을 200으로 결정하였다.\n",
    "\n",
    "이후 나는 기존에 학습이 되는 속도가 느렸던 것이 낮은 정확도의 원인이라고 생각했고 learning rate와 epoch를 모두 올려보기로 하였다.\n",
    "Learning rate가 너무 클 경우 학습이 잘 되지 않지만 적당한 경우 학습 속도를 늘릴 수 있다. learning rate가 1.5 이상 일 때는 epoch가 100회에 도달할 때까지도 학습이 잘 되지 않는 것을 확인했다. 이후 learning rate를 1.3으로 변경하자 epoch 100회에서 정확도를 94%수준까지 올릴 수 있는 것을 확인했다.\n",
    "\n",
    "이후 학습 횟수에 해당하는 epoch를 더 늘리면 정확도가 올라갈 것이라고 판단하여 200회로 늘려본 결과 95.6%까지 정확도를 높일 수 있었다. hidden_unit개수와 epoch를 더 늘린다면 96% 이상으로 정확도가 높아지는 것을 확인하였다. 그러나 모델의 속도를 함께 고려하면 효율성이 좋다고 생각되지는 않아 적용하지는 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VoQW2HK4Sj2r"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2018142192_최순규_assignment2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
